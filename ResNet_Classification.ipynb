{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9993b1b",
   "metadata": {},
   "source": [
    "# Recreating \" Melanoma diagnosis using deep learning techniques on dermatoscopic images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601309c",
   "metadata": {},
   "source": [
    "## Imports and Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a5275",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f79d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "#import torchvision\n",
    "#import torchvision.transforms.v2 as v2\n",
    "#from torchvision.transforms.v2 import Lambda\n",
    "#from torchvision.utils import draw_bounding_boxes\n",
    "from torch.utils.data import  DataLoader\n",
    "from dataloader import ISICClassImageDataset\n",
    "from transforms import dataTransforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "device = torch.device(device)\n",
    "\n",
    "from torchvision.models import ResNet152_Weights\n",
    "from torchvision.models import resnet152\n",
    "weights = ResNet152_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f02d6",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8ec638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "#constants:\n",
    "batch_size = 64\n",
    "data_aug_type = \"1\" #what data augmentation schema to train on\n",
    "dataset = \"Dataset_2018\"\n",
    "if data_aug_type == \"5\":\n",
    "    dataset = dataset+\"_ph2\"\n",
    "elif data_aug_type == \"6\":\n",
    "    data_aug_type_temp = \"2\"\n",
    "else:\n",
    "    data_aug_type_temp = data_aug_type\n",
    "\n",
    "size=(224,224)\n",
    "transforms = dataTransforms(data_aug_type,size=size,mask=False)\n",
    "\n",
    "\n",
    "Train_Dataset      = ISICClassImageDataset(dataset,\"Training\",data_aug_type=data_aug_type_temp,size=size,bb_data_type=data_aug_type_temp)#, target_transform=Lambda(lambda y: torch.zeros( 2, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y,dtype=torch.int64), value=1)),size=size)\n",
    "Validation_Dataset = ISICClassImageDataset(dataset,\"Validation\",data_aug_type=\"1\", size=size,bb_data_type=data_aug_type_temp)\n",
    "Test_Dataset       = ISICClassImageDataset(\"Dataset_2017\",\"Test\",data_aug_type=\"1\", size=size,bb_data_type=data_aug_type_temp)\n",
    "\n",
    "data_loader_params = {\n",
    "    'batch_size': batch_size,  # Batch size for data loading\n",
    "    'num_workers': 10,  # Number of subprocesses to use for data loading\n",
    "    'persistent_workers': True,  # If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the worker dataset instances alive.\n",
    "    'pin_memory': True,  # If True, the data loader will copy Tensors into CUDA pinned memory before returning them. Useful when using GPU.\n",
    "    'pin_memory_device': 'cuda' ,  # Specifies the device where the data should be loaded. Commonly set to use the GPU.\n",
    "}\n",
    "train_dataloader      = DataLoader(Train_Dataset, **data_loader_params, shuffle=True)\n",
    "validation_dataloader = DataLoader(Validation_Dataset, **data_loader_params, shuffle=True)\n",
    "test_dataloader       = DataLoader(Test_Dataset, **data_loader_params, shuffle=False,in_order=True)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13be7a3",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd0f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_FREQUENCY = 10\n",
    "def train_one_epoch(model, training_loader, epoch_index, loss_fn, tb_writer, optimizer, lr_scheduler, scaler= None):\n",
    "    \"\"\"\n",
    "    This function will train your model and save the one that perfroms the best of validation data  \n",
    "    model: the model you wish to test  \n",
    "    train_loader: the data loader of the training dataset  \n",
    "    epoch_index: what epoch we are in for reporting purposes  \n",
    "    loss_fn: your chose loss function  \n",
    "    tb_writer: a summaryWriter used for tracking our training  \n",
    "    optimizer: the algorthim to step toward the optimal solution  \n",
    "    lr_scheduler: the lr_scheduler changes the lr dynamically as needed  \n",
    "    scaler: scales the loss dues to our use of mixed precision  \n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, targets = data\n",
    "        #send them to the model's device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #use automatic mixed precision to reduce memory consumption and allow us to run on more limited resources\n",
    "        with torch.amp.autocast(torch.device(device).type):\n",
    "            # Make predictions for this batch\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss and its gradients\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        running_loss += loss\n",
    "        if scaler:#if were scaling our loss\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            old_scaler = scaler.get_scale()\n",
    "            scaler.update()\n",
    "            new_scaler = scaler.get_scale()\n",
    "            if new_scaler >= old_scaler:\n",
    "                lr_scheduler.step()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        \n",
    "        if i % REPORT_FREQUENCY == REPORT_FREQUENCY-1:\n",
    "            last_loss = running_loss / i # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a3893",
   "metadata": {},
   "source": [
    "## ResNet initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749b05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained_Models\\ResNet152_Best_7_0.5\n"
     ]
    }
   ],
   "source": [
    "ResNet_model_train = False\n",
    "fine_tune = False\n",
    "loss_weight = 0.5\n",
    "if dataset == \"Dataset_2018\":\n",
    "    ResNet_model_path = 'Trained_Models\\\\ResNet152_Best_{}_{}'.format(7,loss_weight)\n",
    "else:\n",
    "    ResNet_model_path = 'Trained_Models\\\\ResNet152_Best_{}_{}'.format(data_aug_type,loss_weight)\n",
    "lr = 0.0001\n",
    "EPOCHS = 100\n",
    "ResNet_model = resnet152(weights=weights, progress=False)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor([1-loss_weight,loss_weight],device=device))\n",
    "ResNet_model.fc = nn.Linear(in_features=2048,out_features=2,bias=True)#change our number of classes to 2\n",
    "optimizer = torch.optim.SGD(ResNet_model.parameters(), lr=lr,weight_decay=0.000001)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=EPOCHS*len(train_dataloader))\n",
    "print(ResNet_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2e2dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "if fine_tune:\n",
    "    checkpoint = torch.load(ResNet_model_path)\n",
    "    ResNet_model.load_state_dict(checkpoint)\n",
    "    #change the path of the best model to be model 6 to bring it in line with the paper\n",
    "    ResNet_model_path = 'Trained_Models\\\\ResNet152_Best_{}_{}'.format(6,loss_weight)\n",
    "if ResNet_model_train:\n",
    "    ResNet_model.to(device)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter('runs/ResNet{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        ResNet_model.train(True)\n",
    "        avg_loss = train_one_epoch(ResNet_model, train_dataloader, epoch_number, loss_fn, writer,optimizer,lr_scheduler,scaler)\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        best_acc = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        true_malig = 0.0\n",
    "        true_benign = 0.0\n",
    "        total_malig = 0.00000001 #prevent divide by zero\n",
    "        total_benign = 0.0          \n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        ResNet_model.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(validation_dataloader):\n",
    "                vinputs, vtargets = vdata\n",
    "                vinputs = vinputs.to(device)\n",
    "                vtargets = vtargets.to(device)\n",
    "                #again were using AMP to allow us to train faster\n",
    "                with torch.amp.autocast(torch.device(device).type):\n",
    "                    voutputs = ResNet_model(vinputs)\n",
    "                    vloss = loss_fn(voutputs,vtargets)\n",
    "                    running_vloss += vloss\n",
    "                    _, preds = voutputs.max(1)\n",
    "                    _, vtarget = vtargets.max(1)\n",
    "                #print((preds == vtarget).sum(), preds.size(0))\n",
    "                num_correct += (preds == vtarget).sum()\n",
    "                num_samples += preds.size(0)\n",
    "                true_malig   += torch.sum(((preds == 1.0) & (vtarget == 1.0))).item()\n",
    "                true_benign  += torch.sum(((preds == 0.0) & (vtarget == 0.0))).item()\n",
    "                total_malig  += torch.sum(preds == 1.0).item()\n",
    "                total_benign += torch.sum(preds == 0.0).item()\n",
    "                #print(true_benign,total_benign)\n",
    "                #print(true_malig,total_malig)\n",
    "                \n",
    "                \n",
    "        acc = float(num_correct) / num_samples\n",
    "        bal_acc = 0.5*((true_malig/total_malig)+(true_benign/total_benign))\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        print('Accuracy:', acc, \"Balanced Accuracy:\", bal_acc)\n",
    "        # Log the running loss averaged per batch\n",
    "        # for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                        epoch_number + 1)\n",
    "        writer.add_scalar(\"Balanced Accuracy\",bal_acc,epoch_number+1)\n",
    "        writer.flush()\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        #AKA early stopping, a form of regularization we talked about it class\n",
    "        if bal_acc > best_acc:\n",
    "            best_acc = bal_acc\n",
    "            torch.save(ResNet_model.state_dict(), \"{}\".format(ResNet_model_path))\n",
    "\n",
    "        epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7ed96",
   "metadata": {},
   "source": [
    "Here we can load our lowest loss model that we trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2139721",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ResNet_model_train:\n",
    "    \n",
    "    checkpoint = torch.load(ResNet_model_path)\n",
    "    ResNet_model.load_state_dict(checkpoint)\n",
    "    ResNet_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fdd3b",
   "metadata": {},
   "source": [
    "### Testing our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "069e386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933333333333333\n",
      "Balanced Accuracy: 0.6204333848273559\n",
      "Precision: 0.4186046511627907\n",
      "Recall: 0.15384615384615385\n",
      "F1-Score: 0.22500000000000003\n",
      "Specificity: 0.94824016563147\n"
     ]
    }
   ],
   "source": [
    "ResNet_model.eval()\n",
    "running_loss = 0.0\n",
    "best_acc = 0.0\n",
    "num_correct = 0.0\n",
    "num_samples = 0.0\n",
    "true_malig = 0.0\n",
    "true_benign = 0.0\n",
    "false_malig = 0.0\n",
    "false_benign = 0.0\n",
    "total_malig = 0.0\n",
    "total_benign = 0.0\n",
    "# Disable gradient computation and reduce memory consumption.\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(test_dataloader):\n",
    "        inputs, targets = vdata\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        #again were using AMP to allow us to train faster\n",
    "        outputs = ResNet_model(inputs)\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        running_loss += loss\n",
    "        _, preds = outputs.max(1)\n",
    "        _, target = targets.max(1)\n",
    "        num_correct += torch.sum(preds == target).item()\n",
    "        num_samples += preds.size(0)\n",
    "        #print(((preds == 0.0) == (target == 0.0)) == 0.0)\n",
    "        true_malig   += torch.sum(((preds == 1.0) & (target == 1.0))).item()\n",
    "        true_benign  += torch.sum(((preds == 0.0) & (target == 0.0))).item()\n",
    "        false_malig   += torch.sum(((preds == 1.0) & (target == 0.0))).item()\n",
    "        false_benign  += torch.sum(((preds == 0.0) & (target == 1.0))).item()\n",
    "        total_malig  += torch.sum(preds == 1.0).item()\n",
    "        total_benign += torch.sum(preds == 0.0).item()\n",
    "    acc = (num_correct / num_samples)\n",
    "precision = true_malig/(true_malig+false_malig)\n",
    "recall = true_malig/(true_malig+false_benign)\n",
    "f1 = (2*precision*recall)/(precision+recall)\n",
    "specificity = true_benign/(true_benign+false_malig)\n",
    "print('Accuracy:', acc)\n",
    "print('Balanced Accuracy:', 0.5*((true_malig/total_malig)+(true_benign/total_benign)))\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-Score:', f1)\n",
    "print('Specificity:', specificity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
